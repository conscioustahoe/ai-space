{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3a4c4f",
   "metadata": {},
   "source": [
    "# Autodiff and Backpropagation\n",
    "\n",
    "## Jacobian\n",
    "\n",
    "Let ${\\bf f}:\\mathbb{R}^n\\to \\mathbb{R}^m$, we define its Jacobian as:\n",
    "\\begin{align*}\n",
    "\\newcommand{\\bbx}{{\\bf x}}\n",
    "\\newcommand{\\bbv}{{\\bf v}}\n",
    "\\newcommand{\\bbw}{{\\bf w}}\n",
    "\\newcommand{\\bbu}{{\\bf u}}\n",
    "\\newcommand{\\bbf}{{\\bf f}}\n",
    "\\newcommand{\\bbg}{{\\bf g}}\n",
    "\\frac{\\partial \\bbf}{\\partial \\bbx} = J_{\\bbf}(\\bbx) &= \\left( \\begin{array}{ccc}\n",
    "\\frac{\\partial f_1}{\\partial x_1}&\\dots& \\frac{\\partial f_1}{\\partial x_n}\\\\\n",
    "\\vdots&&\\vdots\\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1}&\\dots& \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{array}\\right)\\\\\n",
    "&=\\left( \\frac{\\partial \\bbf}{\\partial x_1},\\dots, \\frac{\\partial \\bbf}{\\partial x_n}\\right)\\\\\n",
    "&=\\left(\n",
    "\\begin{array}{c}\n",
    "\\nabla f_1(\\bbx)^T\\\\\n",
    "\\vdots\\\\\n",
    "\\nabla f_m(x)^T\n",
    "\\end{array}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Hence the Jacobian $J_{\\bbf}(\\bbx)\\in \\mathbb{R}^{m\\times n}$ is a linear map from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ such that for $\\bbx,\\bbv \\in \\mathbb{R}^n$ and $h\\in \\mathbb{R}$:\n",
    "\\begin{align*}\n",
    "\\bbf(\\bbx+h\\bbv) = \\bbf(\\bbx) + hJ_{\\bbf}(\\bbx)\\bbv +o(h).\n",
    "\\end{align*}\n",
    "The term $J_{\\bbf}(\\bbx)\\bbv\\in \\mathbb{R}^m$ is a Jacobian Vector Product (**JVP**), correponding to the interpretation where the Jacobian is the linear map: $J_{\\bbf}(\\bbx):\\mathbb{R}^n \\to \\mathbb{R}^m$, where $J_{\\bbf}(\\bbx)(\\bbv)=J_{\\bbf}(\\bbx)\\bbv$.\n",
    "\n",
    "## Chain composition\n",
    "\n",
    "In machine learning, we are computing gradient of the loss function with respect to the parameters. In particular, if the parameters are high-dimensional, the loss is a real number. Hence, consider a real-valued function $\\bbf:\\mathbb{R}^n\\stackrel{\\bbg_1}{\\to}\\mathbb{R}^m \\stackrel{\\bbg_2}{\\to}\\mathbb{R}^d\\stackrel{h}{\\to}\\mathbb{R}$, so that $\\bbf(\\bbx) = h(\\bbg_2(\\bbg_1(\\bbx)))\\in \\mathbb{R}$. We have\n",
    "\\begin{align*}\n",
    "\\underbrace{\\nabla\\bbf(\\bbx)}_{n\\times 1}=\\underbrace{J_{\\bbg_1}(\\bbx)^T}_{n\\times m}\\underbrace{J_{\\bbg_2}(\\bbg_1(\\bbx))^T}_{m\\times d}\\underbrace{\\nabla h(\\bbg_2(\\bbg_1(\\bbx)))}_{d\\times 1}.\n",
    "\\end{align*}\n",
    "To do this computation, if we start from the right so that we start with a matrix times a vector to obtain a vector (of size $m$) and we need to make another matrix times a vector, resulting in $O(nm+md)$ operations. If we start from the left with the matrix-matrix multiplication, we get $O(nmd+nd)$ operations. Hence we see that as soon as $m\\approx d$, starting for the right is much more efficient. Note however that doing the computation from the right to the left requires to keep in memory the values of $\\bbg_1(\\bbx)\\in\\mathbb{R}^m$, and $\\bbx\\in \\mathbb{R}^n$.\n",
    "\n",
    "**Backpropagation** is an efficient algorithm computing the gradient \"from the right to the left\", i.e. backward. In particular, we will need to compute quantities of the form: $J_{\\bbf}(\\bbx)^T\\bbu \\in \\mathbb{R}^n$ with $\\bbu \\in\\mathbb{R}^m$ which can be rewritten $\\bbu^T J_{\\bbf}(\\bbx)$ which is a Vector Jacobian Product (**VJP**), correponding to the interpretation where the Jacobian is the linear map: $J_{\\bbf}(\\bbx):\\mathbb{R}^n \\to \\mathbb{R}^m$, composed with the linear map $\\bbu:\\mathbb{R}^m\\to \\mathbb{R}$ so that $\\bbu^TJ_{\\bbf}(\\bbx) = \\bbu \\circ J_{\\bbf}(\\bbx)$.\n",
    "\n",
    "**example:** let $\\bbf(\\bbx, W) = \\bbx W\\in \\mathbb{R}^b$ where $W\\in \\mathbb{R}^{a\\times b}$ and $\\bbx\\in \\mathbb{R}^a$. We clearly have\n",
    "$$\n",
    "J_{\\bbf}(\\bbx) = W^T.\n",
    "$$\n",
    "Note that here, we are slightly abusing notations and considering the partial function $\\bbx\\mapsto \\bbf(\\bbx, W)$. To see this, we can write $f_j = \\sum_{i}x_iW_{ij}$ so that \n",
    "$$\n",
    "\\frac{\\partial \\bbf}{\\partial x_i}= \\left( W_{i1}\\dots W_{ib}\\right)^T\n",
    "$$\n",
    "Then recall from definitions that\n",
    "$$\n",
    "J_{\\bbf}(\\bbx) = \\left( \\frac{\\partial \\bbf}{\\partial x_1},\\dots, \\frac{\\partial \\bbf}{\\partial x_n}\\right)=W^T.\n",
    "$$\n",
    "Now we clearly have\n",
    "$$\n",
    "J_{\\bbf}(W) = \\bbx \\text{ since, } \\bbf(\\bbx,W+\\Delta W) = \\bbf(\\bbx,W) + \\bbx \\Delta W.\n",
    "$$\n",
    "Note that multiplying $\\bbx$ on the right is actually convenient when using broadcasting, i.e. we can take a batch of input vectors of shape $\\text{bs}\\times a$ without modifying the math above. \n",
    "\n",
    "## Implementation\n",
    "\n",
    "In PyTorch, `torch.autograd` provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. To create a custom [autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function), subclass this class and implement the `forward()` and `backward()` static methods. Here is an example:\n",
    "```python=\n",
    "class Exp(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result\n",
    "# Use it by calling the apply method:\n",
    "output = Exp.apply(input)\n",
    "```\n",
    "\n",
    "\n",
    "### Backprop the functional way\n",
    "\n",
    "Here we will implement in `numpy` a different approach mimicking the functional approach of [JAX](https://jax.readthedocs.io/en/latest/index.html) see [The Autodiff Cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#).\n",
    "\n",
    "Each function will take 2 arguments: one being the input `x` and the other being the parameters `w`. For each function, we build 2 **vjp** functions taking as argument a gradient $\\bbu$ and corresponding to $J_{\\bbf}(\\bbx)$ and $J_{\\bbf}(\\bbw)$ so that these functions return $J_{\\bbf}(\\bbx)^T \\bbu$ and $J_{\\bbf}(\\bbw)^T \\bbu$ respectively. To summarize, for $\\bbx \\in \\mathbb{R}^n$, $\\bbw \\in \\mathbb{R}^d$, and, $\\bbf(\\bbx,\\bbw) \\in \\mathbb{R}^m$,\n",
    "\\begin{align*}\n",
    "{\\bf jvp}_\\bbx(\\bbu) &= J_{\\bbf}(\\bbx)^T \\bbu, \\text{ with } J_{\\bbf}(\\bbx)\\in\\mathbb{R}^{m\\times n}, \\bbu\\in \\mathbb{R}^m\\\\\n",
    "{\\bf jvp}_\\bbw(\\bbu) &= J_{\\bbf}(\\bbw)^T \\bbu, \\text{ with } J_{\\bbf}(\\bbw)\\in\\mathbb{R}^{m\\times d}, \\bbu\\in \\mathbb{R}^m\n",
    "\\end{align*}\n",
    "Then backpropagation is simply done by first computing the gradient of the loss and then composing the **vjp** functions in the right order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81227dfa",
   "metadata": {},
   "source": [
    "### Example: adding bias\n",
    "\n",
    "We start with the simple example of adding a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2159659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7127a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x, b):\n",
    "    return x + b\n",
    "\n",
    "def add_make_vjp(x, b):\n",
    "    def vjp(u):\n",
    "        return u, u\n",
    "    return vjp\n",
    "\n",
    "add.make_vjp = add_make_vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33819c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "x = rng.random((30,2)).astype('float32')\n",
    "b_source  = np.array([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25fbe4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = add(x,b_source)\n",
    "np.allclose(xb, x+b_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3df81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vjp_add = add.make_vjp(x,b_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e7d84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_x, grad_b = vjp_add(rng.random((30,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa080e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699aaccb",
   "metadata": {},
   "source": [
    "### Exercise: dot product and squared loss\n",
    "\n",
    "Implement the corresponding vjp functions. (Note: we are abusing notation for the squared loss as the target `y` is not a parameter and should not be updated! Moreover, the vjp_{y_pred} function for the squared loss does not depend on its input `u`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e64a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, W):\n",
    "    return np.dot(x, W)\n",
    "\n",
    "def dot_make_vjp(x, W):\n",
    "    def vjp(u):\n",
    "        return np.dot(u, W.T), np.einsum('na,nb-> nab',x , u)\n",
    "    return vjp\n",
    "\n",
    "dot.make_vjp = dot_make_vjp\n",
    "\n",
    "def squared_loss(y_pred, y):\n",
    "    return np.array([np.sum((y - y_pred) ** 2)])\n",
    "\n",
    "def squared_loss_make_vjp(y_pred, y):\n",
    "    def vjp(u):\n",
    "        diff = y_pred - y\n",
    "        return 2*diff, np.zeros_like(y)\n",
    "    return vjp\n",
    "\n",
    "squared_loss.make_vjp = squared_loss_make_vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790d7e4",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Our model is:\n",
    "$$\n",
    "y_t = 2x^1_t-3x^2_t+1, \\quad t\\in\\{1,\\dots,30\\}\n",
    "$$\n",
    "\n",
    "Our task is given the 'observations' $(x_t,y_t)_{t\\in\\{1,\\dots,30\\}}$ to recover the weights $w^1=2, w^2=-3$ and the bias $b = 1$.\n",
    "\n",
    "In order to do so, we will solve the following optimization problem:\n",
    "$$\n",
    "\\underset{w^1,w^2,b}{\\operatorname{argmin}} \\sum_{t=1}^{30} \\left(w^1x^1_t+w^2x^2_t+b-y_t\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1bddb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "# generate random input data\n",
    "x = rng.random((30,2)).astype('float32')\n",
    "# generate labels corresponding to input data x\n",
    "y = np.dot(x, [2., -3.]) + 1.\n",
    "y = np.expand_dims(y, axis=1).astype('float32')\n",
    "w_source = np.array([2., -3.])\n",
    "b_source  = np.array([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d172f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feed_forward(y, seed=0):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    funcs = [dot,add,squared_loss]\n",
    "    params = [rng.randn(2,1),rng.randn(1),y]\n",
    "    return funcs, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39fa69",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "The following function should take a batch of inputs, functions and their parameters and return the final value or all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b1d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chain(x, funcs, params, return_all=False):\n",
    "    all_x = [x]\n",
    "    for (f,p) in zip(funcs,params):\n",
    "        x = f(all_x[-1],p)\n",
    "        all_x.append(x)\n",
    "    \n",
    "    if return_all:\n",
    "        return all_x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5a22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs, params = create_feed_forward(y=y, seed=0)\n",
    "W, b, _ = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c76a3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = evaluate_chain(x, funcs, params, return_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eda521",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "The following function should do the forward pass and then the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b456409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_diff_chain(x, funcs, params):\n",
    "    \"\"\"\n",
    "    Reverse-mode differentiation of a chain of computations.\n",
    "\n",
    "    Args:\n",
    "    x: initial input to the chain.\n",
    "    funcs: a list of functions of the form func(x, param).\n",
    "    params: a list of parameters, with len(params) = len(funcs).\n",
    "    Returns:\n",
    "    value, vjp_x, all vjp_params\n",
    "    \"\"\"\n",
    "    # Evaluate the feedforward model and store intermediate computations,\n",
    "    # as they will be needed during the backward pass.\n",
    "    xs = evaluate_chain(x, funcs, params, return_all=True)\n",
    "    K = len(funcs)  # Number of functions.\n",
    "    u = None # the gradient of the loss does not require an input    \n",
    "    # List that will contain the Jacobian of each function w.r.t. parameters.\n",
    "    J = [None] * K\n",
    "\n",
    "    for (k,(f,p,x)) in reversed(list(enumerate(zip(funcs,params,xs)))):\n",
    "        vjp_x, vjp_param = f.make_vjp(x,p)(u)\n",
    "        u = vjp_x\n",
    "        J[k] = vjp_param\n",
    "\n",
    "    return xs[-1], u, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6017ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grad_x, grads = backward_diff_chain(x, funcs, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04d555",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "First compute the update for each parameter and then modify the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbb8e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_SGD(grads, learning_rate = 1e-2):\n",
    "    return [-learning_rate*g.sum(0) for i,g in enumerate(grads)]\n",
    "\n",
    "def update_params(updates, params):\n",
    "    return [params[i] + u for i,u in enumerate(updates)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24342325",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d733022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: epoch: 0 loss [110.2900527]\n",
      "progress: epoch: 1 loss [17.17577308]\n",
      "progress: epoch: 2 loss [15.48445439]\n",
      "progress: epoch: 3 loss [14.27434616]\n",
      "progress: epoch: 4 loss [13.16495088]\n",
      "progress: epoch: 5 loss [12.14630386]\n",
      "progress: epoch: 6 loss [11.21068817]\n",
      "progress: epoch: 7 loss [10.35106573]\n",
      "progress: epoch: 8 loss [9.56101153]\n",
      "progress: epoch: 9 loss [8.83465916]\n",
      "estimation of the parameters:\n",
      "[array([[ 1.6269806 ],\n",
      "       [-1.09195793]]), array([0.121909])]\n"
     ]
    }
   ],
   "source": [
    "funcs, params = create_feed_forward(y=y, seed=0)\n",
    "W, b, _ = params\n",
    "for epoch in range(10):\n",
    "    loss, grad_x, grads = backward_diff_chain(x, funcs, params)\n",
    "    print(\"progress:\", \"epoch:\", epoch, \"loss\",loss)\n",
    "    updates = optim_SGD(grads)\n",
    "    params = update_params(updates, params)\n",
    "    \n",
    "# After training\n",
    "print(\"estimation of the parameters:\")\n",
    "print(params[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775a177",
   "metadata": {},
   "source": [
    "### Jax implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "335ee8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: epoch: 0 loss 110.29005\n",
      "progress: epoch: 1 loss 17.175777\n",
      "progress: epoch: 2 loss 15.484457\n",
      "progress: epoch: 3 loss 14.274347\n",
      "progress: epoch: 4 loss 13.164951\n",
      "progress: epoch: 5 loss 12.146305\n",
      "progress: epoch: 6 loss 11.21069\n",
      "progress: epoch: 7 loss 10.351067\n",
      "progress: epoch: 8 loss 9.561012\n",
      "progress: epoch: 9 loss 8.83466\n",
      "estimation of the parameters:\n",
      "FlatMap({\n",
      "  'linear': FlatMap({\n",
      "              'b': DeviceArray([0.12190904], dtype=float32),\n",
      "              'w': DeviceArray([[ 1.6269805],\n",
      "                                [-1.0919579]], dtype=float32),\n",
      "            }),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "from functools import partial\n",
    "\n",
    "class config:\n",
    "    size_out = 1\n",
    "    w_source = jnp.array(W)\n",
    "    b_source = jnp.array(b)\n",
    "    \n",
    "def _linear(x, config):\n",
    "    return hk.Linear(config.size_out,w_init=hk.initializers.Constant(config.w_source), b_init=hk.initializers.Constant(config.b_source))(x)\n",
    "\n",
    "def mse_loss(y_pred, y_t):\n",
    "    return jax.lax.integer_pow(y_pred - y_t,2).sum()\n",
    "\n",
    "def loss_fn(x_in, y_t, config):\n",
    "    return mse_loss(_linear(x=x_in, config=config),y_t)\n",
    "\n",
    "hk_loss_fn = hk.without_apply_rng(hk.transform(partial(loss_fn, config=config)))\n",
    "params = hk_loss_fn.init(x_in=x,y_t=y,rng=None)\n",
    "loss_fn = hk_loss_fn.apply\n",
    "\n",
    "optimizer = optax.sgd(learning_rate=1e-2)\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "for epoch in range(10):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params,x_in=x,y_t=y)\n",
    "    print(\"progress:\", \"epoch:\", epoch, \"loss\",loss)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "# After training\n",
    "print(\"estimation of the parameters:\")\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e39d5e",
   "metadata": {},
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29e71f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: epoch: 0 loss 110.2900619506836\n",
      "progress: epoch: 1 loss 17.1757755279541\n",
      "progress: epoch: 2 loss 15.484455108642578\n",
      "progress: epoch: 3 loss 14.274348258972168\n",
      "progress: epoch: 4 loss 13.164952278137207\n",
      "progress: epoch: 5 loss 12.146303176879883\n",
      "progress: epoch: 6 loss 11.210689544677734\n",
      "progress: epoch: 7 loss 10.351065635681152\n",
      "progress: epoch: 8 loss 9.561013221740723\n",
      "progress: epoch: 9 loss 8.834660530090332\n",
      "estimation of the parameters:\n",
      "Parameter containing:\n",
      "tensor([[ 1.6270, -1.0920]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1219], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "w_init_t = torch.from_numpy(W).type(dtype)\n",
    "b_init_t = torch.from_numpy(b).type(dtype)\n",
    "x_t = torch.from_numpy(x).type(dtype)\n",
    "y_t = torch.from_numpy(y).type(dtype)\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(2, 1),)\n",
    "\n",
    "for m in model.children():\n",
    "    m.weight.data = w_init_t.T.clone()\n",
    "    m.bias.data = b_init_t.clone()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(10):\n",
    "    y_pred = model(x_t)\n",
    "    loss = loss_fn(y_pred, y_t)\n",
    "    print(\"progress:\", \"epoch:\", epoch, \"loss\",loss.item())\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# After training\n",
    "print(\"estimation of the parameters:\")\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
