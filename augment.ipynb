{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pickle\n",
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter, itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# External Libraries\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from fastcore.test import test_close\n",
    "\n",
    "# Torch-related Libraries\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, tensor, optim, distributions\n",
    "from torch.nn import init\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "# Torch configuration\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "\n",
    "# External Datasets\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "# Project-Specific Modules (miniai)\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Limit CPUs for fastcore\n",
    "if fc.defaults.cpus > 8: \n",
    "    fc.defaults.cpus = 8\n",
    "\n",
    "# Disable logging warnings\n",
    "logging.disable(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 1024\n",
    "xmean,xstd = 0.28, 0.35\n",
    "\n",
    "@inplace\n",
    "def transformi(b):\n",
    "    b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n",
    "\n",
    "dsd = load_dataset(name)\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MulticlassAccuracy())\n",
    "astats = ActivationStats(fc.risinstance(GeneralRelu))\n",
    "cbs = [DeviceCB(), metrics, ProgressCB(plot=True), astats]\n",
    "act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "iw = partial(init_weights, leaky=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "lr,epochs = 6e-2,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going wider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.Flatten(), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.mean((-2,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model2(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [ResBlock(256, 512, act=act, norm=norm), GlobalAvgPool()]\n",
    "    layers += [nn.Linear(512, 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flops(x, h, w):\n",
    "    if x.dim()<3:\n",
    "        return x.numel()\n",
    "    \n",
    "    if x.dim()==4:\n",
    "        return x.numel()*h*w\n",
    "\n",
    "@fc.patch\n",
    "def summary(self:Learner):\n",
    "    res = '|Module|Input|Output|Num params|MFLOPS|\\n|--|--|--|--|--|\\n'\n",
    "    totp,totf = 0,0\n",
    "    def _f(hook, mod, inp, outp):\n",
    "        nonlocal res,totp,totf\n",
    "        nparms = sum(o.numel() for o in mod.parameters())\n",
    "        totp += nparms\n",
    "        *_,h,w = outp.shape\n",
    "        flops = sum(_flops(o, h, w) for o in mod.parameters())/1e6\n",
    "        totf += flops\n",
    "        res += f'|{type(mod).__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{nparms}|{flops:.1f}|\\n'\n",
    "    with Hooks(self.model, _f) as hooks: self.fit(1, lr=1, cbs=SingleBatchCB())\n",
    "    print(f\"Tot params: {totp}; MFLOPS: {totf:.1f}\")\n",
    "    if fc.IN_NOTEBOOK:\n",
    "        from IPython.display import Markdown\n",
    "        return Markdown(res)\n",
    "    else: print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model2(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model3(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [GlobalAvgPool(), nn.Linear(256, 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model3(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in get_model3()[0].parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model3(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model4(act=nn.ReLU, nfs=(16,32,64,128,256), norm=nn.BatchNorm2d):\n",
    "    layers = [conv(1, 16, ks=5, stride=1, act=act, norm=norm)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [GlobalAvgPool(), nn.Linear(256, 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.shape for o in get_model4()[0].parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLearner(get_model4(), dls, F.cross_entropy, lr=lr, cbs=[DeviceCB()]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = get_model4(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "After 20 epochs without augmentation:\n",
    "\n",
    "```\n",
    "{'accuracy': '0.999', 'loss': '0.012', 'epoch': 19, 'train': True}\n",
    "{'accuracy': '0.924', 'loss': '0.284', 'epoch': 19, 'train': False}\n",
    "```\n",
    "\n",
    "With batchnorm, weight decay doesn't really regularize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_batch(b, tfm_x=fc.noop, tfm_y = fc.noop): return tfm_x(b[0]),tfm_y(b[1])\n",
    "\n",
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=4),\n",
    "                     transforms.RandomHorizontalFlip())\n",
    "\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)\n",
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[SingleBatchCB(), augcb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = learn.batch\n",
    "show_images(xb[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "@fc.delegates(show_images)\n",
    "def show_image_batch(self:Learner, max_n=9, cbs=None, **kwargs):\n",
    "    self.fit(1, cbs=[SingleBatchCB()]+fc.L(cbs))\n",
    "    show_images(self.batch[0][:max_n], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_image_batch(max_n=16, imsize=(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom collation function could let you do per-item transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = Path('models')\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "torch.save(learn.model, mdl_path/'data_aug.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test time augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapturePreds(Callback):\n",
    "    def before_fit(self, learn): self.all_inps,self.all_preds,self.all_targs = [],[],[]\n",
    "    def after_batch(self, learn):\n",
    "        self.all_inps. append(to_cpu(learn.batch[0]))\n",
    "        self.all_preds.append(to_cpu(learn.preds))\n",
    "        self.all_targs.append(to_cpu(learn.batch[1]))\n",
    "    def after_fit(self, learn):\n",
    "        self.all_preds,self.all_targs,self.all_inps = map(torch.cat, [self.all_preds,self.all_targs,self.all_inps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fc.patch\n",
    "def capture_preds(self: Learner, cbs=None, inps=False):\n",
    "    cp = CapturePreds()\n",
    "    self.fit(1, train=False, cbs=[cp]+fc.L(cbs))\n",
    "    res = cp.all_preds,cp.all_targs\n",
    "    if inps: res = res+(cp.all_inps,)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap1, at = learn.capture_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttacb = BatchTransformCB(partial(tfm_batch, tfm_x=TF.hflip), on_val=True)\n",
    "ap2, at = learn.capture_preds(cbs=[ttacb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap1.shape,ap2.shape,at.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = torch.stack([ap1,ap2]).mean(0).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((ap==at).float().mean().item(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random erase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rand_erase1(x, pct, xm, xs, mn, mx):\n",
    "    szx = int(pct*x.shape[-2])\n",
    "    szy = int(pct*x.shape[-1])\n",
    "    stx = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    init.normal_(x[:,:,stx:stx+szx,sty:sty+szy], mean=xm, std=xs)\n",
    "    x.clamp_(mn, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_erase(x, pct=0.2, max_num = 4):\n",
    "    xm,xs,mn,mx = x.mean(),x.std(),x.min(),x.max()\n",
    "    num = random.randint(0, max_num)\n",
    "    for i in range(num): _rand_erase1(x, pct, xm, xs, mn, mx)\n",
    "#     print(num)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandErase(nn.Module):\n",
    "    def __init__(self, pct=0.2, max_num=4):\n",
    "        super().__init__()\n",
    "        self.pct,self.max_num = pct,max_num\n",
    "    def forward(self, x): return rand_erase(x, self.pct, self.max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     RandErase())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rand_copy1(x, pct):\n",
    "    szx = int(pct*x.shape[-2])\n",
    "    szy = int(pct*x.shape[-1])\n",
    "    stx1 = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty1 = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    stx2 = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty2 = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    x[:,:,stx1:stx1+szx,sty1:sty1+szy] = x[:,:,stx2:stx2+szx,sty2:sty2+szy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_copy(x, pct=0.2, max_num = 4):\n",
    "    num = random.randint(0, max_num)\n",
    "    for i in range(num): _rand_copy1(x, pct)\n",
    "#     print(num)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandCopy(nn.Module):\n",
    "    def __init__(self, pct=0.2, max_num=4):\n",
    "        super().__init__()\n",
    "        self.pct,self.max_num = pct,max_num\n",
    "    def forward(self, x): return rand_copy(x, self.pct, self.max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = nn.Sequential(transforms.RandomCrop(28, padding=1),\n",
    "                     transforms.RandomHorizontalFlip(),\n",
    "                     RandCopy())\n",
    "augcb = BatchTransformCB(partial(tfm_batch, tfm_x=tfms), on_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=[DeviceCB(), SingleBatchCB(), augcb])\n",
    "learn.fit(1)\n",
    "xb,yb = learn.batch\n",
    "show_images(xb[:16], imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(1)\n",
    "epochs = 25\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn2 = TrainLearner(model2, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn2.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = Path('models')\n",
    "torch.save(learn.model,  mdl_path/'randcopy1.pkl')\n",
    "torch.save(learn2.model, mdl_path/'randcopy2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp1 = CapturePreds()\n",
    "learn.fit(1, train=False, cbs=cp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp2 = CapturePreds()\n",
    "learn2.fit(1, train=False, cbs=cp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = torch.stack([cp1.all_preds,cp2.all_preds]).mean(0).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((ap==cp1.all_targs).float().mean().item(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = 0.1\n",
    "dist = distributions.binomial.Binomial(probs=1-p)\n",
    "dist.sample((10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        dist = distributions.binomial.Binomial(tensor(1.0).to(x.device), probs=1-self.p)\n",
    "        return x * dist.sample(x.size()) * 1/(1-self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropmodel(act=nn.ReLU, nfs=(16,32,64,128,256,512), norm=nn.BatchNorm2d, drop=0.0):\n",
    "    layers = [ResBlock(1, 16, ks=5, stride=1, act=act, norm=norm), nn.Dropout2d(drop)]\n",
    "    layers += [ResBlock(nfs[i], nfs[i+1], act=act, norm=norm, stride=2) for i in range(len(nfs)-1)]\n",
    "    layers += [nn.Flatten(), Dropout(drop), nn.Linear(nfs[-1], 10, bias=False), nn.BatchNorm1d(10)]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "epochs=5\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]\n",
    "model = get_dropmodel(act_gr, norm=nn.BatchNorm2d, drop=0.1).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTD_CB(Callback):\n",
    "    def before_epoch(self, learn):\n",
    "        learn.model.apply(lambda m: m.train() if isinstance(m, (nn.Dropout,nn.Dropout2d)) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b):\n",
    "    b[xl] = [(TF.to_tensor(o)*2-1) for o in b[xl]]\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched), augcb]\n",
    "model = get_model(act_gr, norm=nn.BatchNorm2d).apply(iw)\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)\n",
    "learn.fit(epochs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
