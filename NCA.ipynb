{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,random,timm,torchvision,io,PIL, einops\n",
    "import fastcore.all as fc,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "from datasets import load_dataset,load_dataset_builder\n",
    "from fastcore.foundation import L, store_attr\n",
    "\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Cellular Automata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cellular automaton is a discrete model that consists of a grid of cells, each of which can be in one of a finite number of states. The cells are updated simultaneously based on a set of rules that determine the state of a cell based on the states of its neighbors.\n",
    "\n",
    "Cellular automata are often used to model complex systems and can exhibit emergent behavior, meaning that patterns and behaviors emerge from the interactions of the individual cells even though the rules governing their behavior are simple.\n",
    "\n",
    "Classic examples such as the famous 'Game of Life' have very simple rules and limit states to 'alive' or 'dead'. However, the ideas can be extended to continuous outputs for each cell, and the update 'rule' can be a small neural network rather than a hard-coded decision tree - giving us 'Neural Cellular Automata'.\n",
    "\n",
    "Here's what our NCA will look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nca.png](images/nca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key references: \n",
    "- [Growing Neural Cellular Automata](https://distill.pub/2020/growing-ca/) - A delightful paper that was my intro to the topic\n",
    "\n",
    "- [Texture Generation with NCA](https://arxiv.org/abs/2105.07299) - tiny models making amazing textures, the paper we're replicating today.\n",
    "\n",
    "- ['The Future of Artificial Intelligence is Self-Organizing and Self-Assembling'](https://sebastianrisi.com/self_assembling_ai/) - More general discussion of this space\n",
    "\n",
    "- [Fixing Neural CA Colors with Sliced Optimal Transport](https://www.youtube.com/watch?v=ZFYZFlY7lgI) -  A follow-on video from Alexander Mordvintsev (include code for a different style loss)\n",
    "\n",
    "- [Fun with Neural Cellular Automata](https://wandb.ai/johnowhitaker/nca/reports/Fun-with-Neural-Cellular-Automata--VmlldzoyMDQ5Mjg0) - My W&B report with lots of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match This Style with an NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    imgb = fc.urlread(url, decode=False) \n",
    "    return torchvision.io.decode_image(tensor(list(imgb), dtype=torch.uint8)).float()/255.\n",
    "\n",
    "url = \"https://images.pexels.com/photos/34225/spider-web-with-water-beads-network-dewdrop.jpg?w=256\"\n",
    "style_im = download_image(url).to(def_device)\n",
    "show_image(style_im);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = timm.create_model('vgg16', pretrained=True).to(def_device).features\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def calc_features(imgs, target_layers=[18, 25]): \n",
    "    x = normalize(imgs)\n",
    "    feats = []\n",
    "    for i, layer in enumerate(vgg16[:max(target_layers)+1]):\n",
    "        x = layer(x)\n",
    "        if i in target_layers:\n",
    "            feats.append(x.clone())\n",
    "    return feats\n",
    "\n",
    "def calc_grams(img, target_layers=[1, 6, 11, 18, 25]):\n",
    "    return L(torch.einsum('bchw, bdhw -> cd', x, x) / (x.shape[-2]*x.shape[-1]) for x in calc_features(img, target_layers))\n",
    "\n",
    "class StyleLossToTarget():\n",
    "    def __init__(self, target_im, target_layers=[1, 6, 11, 18, 25]):\n",
    "        fc.store_attr()\n",
    "        with torch.no_grad(): \n",
    "            self.target_grams = calc_grams(target_im[None], target_layers)\n",
    "    def __call__(self, input_im): \n",
    "        return sum((f1-f2).pow(2).mean() for f1, f2 in \n",
    "               zip(calc_grams(input_im, self.target_layers), self.target_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_loss = StyleLossToTarget(style_im)\n",
    "style_loss(torch.rand(1, 3, 256, 256).to(def_device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining NCA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_channels = 4\n",
    "hidden_n = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grids(n, sz=128):\n",
    "    return torch.zeros(n, num_channels, sz, sz).to(def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.stack([\n",
    "    tensor([[0.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,0.0]]),\n",
    "    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]),\n",
    "    tensor([[-1.0,0.0,1.0],[-2.0,0.0,2.0],[-1.0,0.0,1.0]]).T,\n",
    "    tensor([[1.0,2.0,1.0],[2.0,-12,2.0],[1.0,2.0,1.0]])\n",
    "]).to(def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perchannel_conv(x, filters):\n",
    "    '''filters: [filter_n, h, w]'''\n",
    "    b, ch, h, w = x.shape\n",
    "    y = x.reshape(b*ch, 1, h, w)\n",
    "    y = F.pad(y, [1, 1, 1, 1], 'circular') # << Note pad mode\n",
    "    y = F.conv2d(y, filters[:,None])\n",
    "    return y.reshape(b, -1, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = make_grids(1)\n",
    "model_inputs = perchannel_conv(x, filters)\n",
    "model_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = nn.Sequential(\n",
    "    nn.Linear(num_channels*4, hidden_n),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_n, num_channels, bias=False)\n",
    ").to(def_device)\n",
    "model_inputs_flat = einops.rearrange(model_inputs, 'b c h w -> (b h w) c') # (1*128*128, 16)\n",
    "brain_preds = brain(model_inputs_flat).reshape(x.shape)\n",
    "brain_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.shape for p in brain.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = nn.Sequential(\n",
    "    nn.Conv2d(num_channels*4, hidden_n, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n",
    ").to(def_device)\n",
    "brain_preds = brain(model_inputs).reshape(x.shape)\n",
    "brain_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p.shape for p in brain.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this into a class, with a few extra features:\n",
    "- Random update: only update ~50% of the cells \n",
    "- to_rgb function to scale and show the first 3 channels as an RGB image\n",
    "- An option to zero out the weights of the second layer. Think: why is this useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCA(nn.Module):\n",
    "    def __init__(self, zero_w2=True):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Conv2d(num_channels*4, hidden_n, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.w2 = nn.Conv2d(hidden_n, num_channels, 1, bias=False)\n",
    "        if zero_w2: self.w2.weight.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x, update_rate=0.5):\n",
    "        y = perchannel_conv(x, filters) # Apply the filters\n",
    "        y = self.w2(self.relu(self.w1(y))) # pass the result through our 'brain'\n",
    "        b, c, h, w = y.shape\n",
    "        update_mask = (torch.rand(b, 1, h, w).to(x.device)+update_rate).floor() # Random update\n",
    "        return x+y*update_mask\n",
    "\n",
    "    def to_rgb(self, x):\n",
    "        return x[...,:3,:,:]+0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LengthDataset():\n",
    "    def __init__(self, length=1): self.length=length\n",
    "    def __len__(self): return self.length\n",
    "    def __getitem__(self, idx): return 0,0\n",
    "\n",
    "def get_dummy_dls(length=100):\n",
    "    return DataLoaders(DataLoader(LengthDataset(length), batch_size=1),\n",
    "                       DataLoader(LengthDataset(1), batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCAProgressCB(ProgressCB):\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if not (hasattr(learn, 'metrics') and learn.training): return \n",
    "        self.losses.append(learn.loss.item())\n",
    "        mbar = self.mbar\n",
    "        if not hasattr(mbar, 'graph_fig'):\n",
    "            mbar.graph_fig, mbar.graph_axs = plt.subplots(1, 2, figsize=(12, 3.5))\n",
    "            mbar.graph_out = display(mbar.graph_fig, display_id=True)\n",
    "\n",
    "        # Update preview image every 64 iters\n",
    "        if (len(self.losses))%64 != 10: return \n",
    "        \n",
    "        # Plot losses:\n",
    "        mbar.graph_axs[0].clear()\n",
    "        mbar.graph_axs[0].plot(self.losses, '.', alpha=0.3)\n",
    "        mbar.graph_axs[0].set_yscale('log')\n",
    "        mbar.graph_axs[0].set_ylim(tensor(self.losses).min(), self.losses[0])\n",
    "        \n",
    "        # Show preview images:\n",
    "        rgb = learn.model.to_rgb(learn.preds.detach()).clip(0, 1)\n",
    "        show_image(torchvision.utils.make_grid(rgb), ax=mbar.graph_axs[1])\n",
    "        \n",
    "        # Update graph\n",
    "        mbar.graph_out.update(mbar.graph_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCACB(TrainCB):\n",
    "    order = DeviceCB.order+1\n",
    "    def __init__(self, ca, style_img_tensor, style_loss_scale=0.1, size=256, \n",
    "                 step_n_min=32, step_n_max=96, batch_size=4):\n",
    "        fc.store_attr()\n",
    "        with torch.no_grad(): self.pool = make_grids(256, sz=size) # Set up a 'pool' of grids\n",
    "    \n",
    "    def predict(self, learn): \n",
    "        \n",
    "        # Pick some random samples from the pool\n",
    "        batch_idx = torch.randint(0, len(self.pool), (self.batch_size,))\n",
    "        x = self.pool[batch_idx]\n",
    "        \n",
    "        # occasionally zero out some samples\n",
    "        if torch.randint(8, (1,)) < 1: \n",
    "            x[:1] =  make_grids(1, sz=self.size)\n",
    "        \n",
    "        # Apply the model a number of times\n",
    "        for _ in range(torch.randint(self.step_n_min, self.step_n_max, (1,))):\n",
    "            x = learn.model(x)\n",
    "        \n",
    "        # Update pool\n",
    "        with torch.no_grad(): self.pool[batch_idx] = x\n",
    "        \n",
    "        # and store preds\n",
    "        learn.preds = x\n",
    "        \n",
    "    def get_loss(self, learn): \n",
    "        style_loss = learn.loss_func(learn.model.to_rgb(self.learn.preds))\n",
    "        overflow_loss = (learn.preds-learn.preds.clamp(-1.0, 1.0)).abs().sum()\n",
    "        learn.loss = overflow_loss + style_loss*self.style_loss_scale\n",
    "        \n",
    "    def backward(self, learn):\n",
    "        learn.loss.backward()\n",
    "        # Gradient normalization:\n",
    "        for p in learn.model.parameters():\n",
    "            p.grad /= (p.grad.norm()+1e-8) \n",
    "        \n",
    "    def before_fit(self, learn): self.learn=learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCA().to(def_device)\n",
    "cbs = [NCACB(model, style_im), NCAProgressCB(), MetricsCB()]\n",
    "style_loss = StyleLossToTarget(style_im)\n",
    "\n",
    "learn = Learner(model, get_dummy_dls(1200), style_loss, lr=1e-3, cbs=cbs, opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = model.to_rgb(learn.preds.detach())\n",
    "rgb = torchvision.utils.make_grid(rgb)\n",
    "show_image(rgb.clip(0, 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "x = torch.randn(1, num_channels, 128, 128).to(def_device) * 0.1\n",
    "for i in range(900):\n",
    "    x = model(x)\n",
    "    if i%100==0: images.append(model.to_rgb(x)[0].clip(0, 1))\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
