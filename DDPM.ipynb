{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239)\n",
    "\n",
    "[Tutorial on Diffusion Models for Imaging and Vision](https://arxiv.org/pdf/2403.18103)\n",
    "\n",
    "[Bayes theorem, the geometry of changing beliefs](https://youtu.be/HZGCoVF3YvM?si=wPw_XLl6pZFQmIws)\n",
    "\n",
    "[Probabilities of probabilities](https://youtube.com/playlist?list=PLZHQObOWTQDOjmo3Y6ADm0ScWAlEXf-fp&si=2GUpUMdHkjuCqSPK)\n",
    "\n",
    "[What are Diffusion Models?](https://youtu.be/fbLgFrlTnGU?si=_d1SbwC5wNQ6eVym)\n",
    "\n",
    "[Diffusion Models | Paper Explanation | Math Explained](https://youtu.be/HoKDTa5jHvg?si=Df3zTMRsPOgloGvI)\n",
    "\n",
    "[The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main components of DDPM\n",
    "\n",
    "#### Forward Diffusion Process\n",
    "\n",
    "The forward diffusion process is about adding noise gradually to the data so that it eventually becomes Gaussian noise. This can be expressed as a series of transitions, where at each step, a small amount of Gaussian noise is added.\n",
    "\n",
    "#### Reverse Process (Denoising)\n",
    "\n",
    "The reverse process is more complex and involves learning how to reverse the noise addition in order to recover the original data. Here, a neural network is typically trained to predict the noise added at each step.\n",
    "\n",
    "#### Connection to Variational Inference (KL Divergence, ELBO, etc.)\n",
    "\n",
    "This part of the model helps explain why and how we can learn the reverse process. We want the reverse process to approximate the true posterior distribution of the data (i.e., the reverse distribution). This can be done by minimizing the KL divergence between the true posterior and the learned reverse process.\n",
    "\n",
    "Great! Letâ€™s start by walking through the **forward diffusion process**, where we gradually add noise to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Process (Diffusion of Data)\n",
    "\n",
    "In the forward process, we transform data $x_0$ (e.g., an image) into a sequence of noisy versions $x_1, x_2, \\ldots, x_T$ through a gradual addition of Gaussian noise. By the end of the process (at time step $T$), the data should resemble pure noise, typically modeled as a Gaussian distribution $\\mathcal{N}(0, I)$.\n",
    "\n",
    "The forward process is defined as a Markov chain, where each state $x_t$ depends only on the previous state $x_{t-1}$. Specifically, we can express this step-wise transition as:\n",
    "\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t} x_{t-1}, (1-\\alpha_t) I)$$\n",
    "\n",
    "Where:\n",
    "- $x_t$ is the noisy data at time step $t$.\n",
    "- $\\alpha_t$ is a variance schedule controlling the amount of noise added at each step.\n",
    "- $\\mathcal{N}(\\mu, \\Sigma)$ denotes a Gaussian distribution with mean $\\mu$ and covariance $\\Sigma$.\n",
    "\n",
    "Let's break this down.\n",
    "\n",
    "#### What's happening in each step?\n",
    "\n",
    "At each step $t$, a small amount of Gaussian noise is added to the previous state $x_{t-1}$. This noise is parameterized by $\\alpha_t$, which controls how much noise is injected at each step. The forward process moves the data from its original clean state $x_0$ towards a fully noisy state $x_T$.\n",
    "\n",
    "- The term $\\sqrt{\\alpha_t} x_{t-1}$ scales down the contribution of the clean data.\n",
    "- The term $(1-\\alpha_t) I$ introduces Gaussian noise.\n",
    "\n",
    "By the time you reach $x_T$, the data has been degraded into noise.\n",
    "\n",
    "#### How do we accumulate noise?\n",
    "\n",
    "We want to describe the relationship between $x_0$ (the original data) and $x_t$ at an arbitrary time step $t$. Since each step involves adding noise, we can compute the cumulative noise added over all the steps up to $t$.\n",
    "\n",
    "By recursively applying the noise-adding process from $t = 1$ to $t = T$, we get:\n",
    "\n",
    "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
    "\n",
    "This equation tells us that $x_t$ is a noisy version of the original data $x_0$, where $\\sqrt{\\bar{\\alpha}_t}$ is a decaying factor applied to the original data, and $1 - \\bar{\\alpha}_t$ governs the amount of noise accumulated over time.\n",
    "\n",
    "#### Why is this form useful?\n",
    "\n",
    "This equation is crucial because it allows us to directly sample $x_t$ at any time step $t$ from the original data $x_0$. The cumulative noise $x_t$ is modeled as a Gaussian distribution where:\n",
    "- The mean is $\\sqrt{\\bar{\\alpha}_t} x_0$, a scaled version of the original data.\n",
    "- The variance is $(1 - \\bar{\\alpha}_t) I$, which grows over time, introducing more noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter, itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Third-party library imports\n",
    "import fastcore.all as fc\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "# torch imports\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor, nn, optim\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "# dataset imports\n",
    "import datasets\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "# miniai imports\n",
    "from miniai.datasets import *\n",
    "from miniai.conv import *\n",
    "from miniai.learner import *\n",
    "from miniai.activations import *\n",
    "from miniai.init import *\n",
    "from miniai.sgd import *\n",
    "from miniai.resnet import *\n",
    "from miniai.augment import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 'image', 'label'\n",
    "dsd = load_dataset('fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b):\n",
    "    b[x] = [TF.resize(TF.to_tensor(o), (32,32)) for o in b[x]] # resize the 28x28 images to 32x32 to make it simpler for model's architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "bs = 128\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=8)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
